{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'ignore_mismatched_sizes': True} are not expected by StableDiffusionInstructPix2PixPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e24af9d0b894a1db74ff7943b0ac256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_instruct_pix2pix.StableDiffusionInstructPix2PixPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "StableDiffusionInstructPix2PixPipeline {\n  \"_class_name\": \"StableDiffusionInstructPix2PixPipeline\",\n  \"_diffusers_version\": \"0.20.0.dev0\",\n  \"_name_or_path\": \"./opt/run-overnight\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"scheduler\": [\n    \"diffusers\",\n    \"PNDMScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n has been incorrectly initialized or <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_instruct_pix2pix.StableDiffusionInstructPix2PixPipeline'> is incorrectly implemented. Expected {'tokenizer', 'feature_extractor', 'unet', 'text_encoder', 'safety_checker', 'vae', 'scheduler'} to be defined, but dict_keys(['vae', 'text_encoder', 'tokenizer', 'unet', 'scheduler', 'feature_extractor']) are defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-05a8ba693485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel_id_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./opt/run-overnight\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStableDiffusionInstructPix2PixPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/4catalyzer-hackathon/diffusers/src/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, torch_device, torch_dtype, silence_dtype_warnings)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;31m# .to(\"cuda\") would raise an error if the pipeline is sequentially offloaded, so we raise our own to make it clearer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         pipeline_is_sequentially_offloaded = any(\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0mmodule_is_sequentially_offloaded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         )\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpipeline_is_sequentially_offloaded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/4catalyzer-hackathon/diffusers/src/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mcomponents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1577\u001b[0m                 \u001b[0;34mf\"{self} has been incorrectly initialized or {self.__class__} is incorrectly implemented. Expected\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m                 \u001b[0;34mf\" {expected_modules} to be defined, but {components.keys()} are defined.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: StableDiffusionInstructPix2PixPipeline {\n  \"_class_name\": \"StableDiffusionInstructPix2PixPipeline\",\n  \"_diffusers_version\": \"0.20.0.dev0\",\n  \"_name_or_path\": \"./opt/run-overnight\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"scheduler\": [\n    \"diffusers\",\n    \"PNDMScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n has been incorrectly initialized or <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_instruct_pix2pix.StableDiffusionInstructPix2PixPipeline'> is incorrectly implemented. Expected {'tokenizer', 'feature_extractor', 'unet', 'text_encoder', 'safety_checker', 'vae', 'scheduler'} to be defined, but dict_keys(['vae', 'text_encoder', 'tokenizer', 'unet', 'scheduler', 'feature_extractor']) are defined."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/ubuntu/4catalyzer-hackathon\")\n",
    "\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
    "\n",
    "device = \"cuda\"\n",
    "# model_id_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "model_id_or_path = \"./opt/run-overnight\"\n",
    "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=False, ignore_mismatched_sizes=True)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "\n",
    "prompts = {\n",
    "    \"h2h\": \"This is an image of a normal brain MRI scan from axial view. It should be grey scale and clearly show brain structure. Do not modify the input image.\",\n",
    "    \"h2uh\": \"Given an input image of a normal brain MRI scan from axial view, modify the image by inserting a brain lesion within the MRI image. This brain lesion is caused by an ischaemic stroke. The lesion should appear relatively lighter compared to its surroundings brain tissue in the image.\",\n",
    "    \"uh2uhm\": \"Given an input image of an unhealthy brain MRI scan from axial view that contains brain lesions, modify the image to colour the lesions light red.\",\n",
    "    \"h2uhm\": \"Given an input image of a normal brain MRI scan from axial view, modify the image by inserting a brain lesion within the MRI image, and colour the lesions light red. This brain lesion is caused by an ischaemic stroke.\",\n",
    "    \"uhm2uhm\": \"This is an image of an unhealthy brain MRI scan from axial view that contains brain lesions. The lesions are coloured light red.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming the 'data' folder is in the same directory as your script\n",
    "data_folder = \"./data/train/\"\n",
    "\n",
    "# # Replace 'sketch-mountains-input.jpg' with the actual filename if needed\n",
    "image_filename = 'healthy_images/sub-ON21834/caseON21834_dwi_slice_40.png'\n",
    "\n",
    "# Replace 'sketch-mountains-input.jpg' with the actual filename if needed\n",
    "# image_filename = 'lesion_images/sub-strokecase0002/case0002_dwi_slice_30.png'\n",
    "\n",
    "# Build the full path to the image file\n",
    "image_path = os.path.join(data_folder, image_filename)\n",
    "\n",
    "# Open and resize the image\n",
    "init_image = Image.open(image_path).convert(\"RGB\")\n",
    "# init_image = init_image.resize((768, 512))\n",
    "\n",
    "# prompt = prompts[\"h2uhm\"]\n",
    "prompt = \"modify the image by inserting brain lesions within the MRI image, and highlight the brain lesions in bright red color.\"\n",
    "\n",
    "generator = torch.Generator(\"cuda\").manual_seed(40)\n",
    "\n",
    "\n",
    "start_steps = 20\n",
    "max_num_inference_steps = 160\n",
    "os.system(\"rm -rf ./viz/\")\n",
    "os.system(\"mkdir ./viz/\")\n",
    "for steps in range(start_steps, max_num_inference_steps, 20):\n",
    "    for guidance_scale in range(2, 15):\n",
    "        healthy_output_file = os.path.basename(image_filename)\n",
    "        output_path = f\"./viz/{healthy_output_file}\"\n",
    "        init_image.save(output_path)\n",
    "\n",
    "        images = pipe(prompt, image=init_image,\n",
    "              num_inference_steps = steps,\n",
    "              image_guidance_scale = 10,\n",
    "              guidance_scale = guidance_scale,\n",
    "              num_images_per_prompt=1,\n",
    "              generator=generator).images\n",
    "        for idx, image in enumerate(images):\n",
    "            pred_output_path = f\"./viz/{os.path.splitext(os.path.basename(image_filename))[0]}-generated-{idx}-step-{steps}-guiding-scale-{guiding_scale}.jpg\"\n",
    "            image.save(pred_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video created successfully at ./viz/video-3.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def create_video_from_directory(image_directory, output_video_path, frame_rate):\n",
    "    image_files = sorted([f for f in os.listdir(image_directory) if f.endswith('.jpg')])  # Modify the extension as needed\n",
    "\n",
    "    if not image_files:\n",
    "        print(\"No image files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    first_image_path = os.path.join(image_directory, image_files[0])\n",
    "    image = cv2.imread(first_image_path)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for the output video, modify as needed\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (width, height))\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_directory, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        out.write(image)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video created successfully at {output_video_path}\")\n",
    "\n",
    "# Example usage:\n",
    "image_directory = './viz/'\n",
    "output_video_path = './viz/video-3.mp4'\n",
    "frame_rate = 1  # Adjust the frame rate as needed\n",
    "\n",
    "create_video_from_directory(image_directory, output_video_path, frame_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
